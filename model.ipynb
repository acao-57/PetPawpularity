{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv('augmented_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the data\n",
    "# Separate features and target\n",
    "df = df.drop(columns=['Id'])\n",
    "\n",
    "features = df.drop(columns=['Pawpularity'])\n",
    "target = df['Pawpularity']\n",
    "\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "features = features.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the labels are in the correct shape\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_val = y_val.values.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1239/1239 [==============================] - 2s 1ms/step - loss: 0.2601 - root_mean_squared_error: 0.5100 - val_loss: 0.0430 - val_root_mean_squared_error: 0.2074 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0450 - root_mean_squared_error: 0.2122 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0428 - root_mean_squared_error: 0.2068 - val_loss: 0.0428 - val_root_mean_squared_error: 0.2068 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0428 - root_mean_squared_error: 0.2069 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2061 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0429 - root_mean_squared_error: 0.2072 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2062 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0429 - root_mean_squared_error: 0.2071 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2059 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0428 - root_mean_squared_error: 0.2070 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2060 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0425 - root_mean_squared_error: 0.2063 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2054 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0425 - root_mean_squared_error: 0.2061 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0424 - root_mean_squared_error: 0.2060 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2056 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0424 - root_mean_squared_error: 0.2059 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0424 - root_mean_squared_error: 0.2058 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0424 - root_mean_squared_error: 0.2058 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2050 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0424 - root_mean_squared_error: 0.2058 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2053 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0423 - root_mean_squared_error: 0.2056 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0422 - root_mean_squared_error: 0.2054 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2049 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0422 - root_mean_squared_error: 0.2053 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0421 - root_mean_squared_error: 0.2052 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0421 - root_mean_squared_error: 0.2052 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2048 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0421 - root_mean_squared_error: 0.2051 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2046 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2050 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0421 - root_mean_squared_error: 0.2051 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2046 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2050 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2043 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2050 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0421 - root_mean_squared_error: 0.2051 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2050 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2050 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2049 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2048 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2045 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "1239/1239 [==============================] - 2s 1ms/step - loss: 0.0420 - root_mean_squared_error: 0.2048 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2048 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2048 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2047 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2048 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "1239/1239 [==============================] - 2s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2043 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2046 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044 - lr: 3.1250e-05\n",
      "Epoch 42/100\n",
      "1239/1239 [==============================] - 1s 1ms/step - loss: 0.0419 - root_mean_squared_error: 0.2047 - val_loss: 0.0417 - val_root_mean_squared_error: 0.2043 - lr: 3.1250e-05\n",
      "310/310 [==============================] - 0s 464us/step\n",
      "Validation RMSE: 0.2043\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='mse', \n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Step 4: Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
