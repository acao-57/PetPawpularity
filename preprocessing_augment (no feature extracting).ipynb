{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "csv_file = 'train.csv'  \n",
    "img_dir = 'train'  \n",
    "output_img_dir = 'augmented_images'  # Folder to save augmented images\n",
    "output_csv_file = 'augmented_train.csv'  # CSV file to save updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv\n",
    "train_df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
      "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
      "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
      "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
      "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
      "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
      "\n",
      "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
      "0          0      1        0      0          0     0     0           63  \n",
      "1          0      0        0      0          0     0     0           42  \n",
      "2          0      0        0      1          1     0     0           28  \n",
      "3          0      0        0      0          0     0     0           15  \n",
      "4          0      1        0      0          0     0     0           72  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9912 entries, 0 to 9911\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Id             9912 non-null   object\n",
      " 1   Subject Focus  9912 non-null   int64 \n",
      " 2   Eyes           9912 non-null   int64 \n",
      " 3   Face           9912 non-null   int64 \n",
      " 4   Near           9912 non-null   int64 \n",
      " 5   Action         9912 non-null   int64 \n",
      " 6   Accessory      9912 non-null   int64 \n",
      " 7   Group          9912 non-null   int64 \n",
      " 8   Collage        9912 non-null   int64 \n",
      " 9   Human          9912 non-null   int64 \n",
      " 10  Occlusion      9912 non-null   int64 \n",
      " 11  Info           9912 non-null   int64 \n",
      " 12  Blur           9912 non-null   int64 \n",
      " 13  Pawpularity    9912 non-null   int64 \n",
      "dtypes: int64(13), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial DataFrame:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "# Checking for any missing values\n",
    "if train_df.isnull().any().any():\n",
    "    print(\"Missing values found:\")\n",
    "    print(train_df.isnull().sum())\n",
    "    \n",
    "else:\n",
    "    print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates\n",
    "duplicates = train_df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"Duplicates found: {duplicates}\")\n",
    "    train_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "else:\n",
    "    print(\"No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pawpularity to float\n",
    "train_df['Pawpularity'] = train_df['Pawpularity'].astype(float)\n",
    "\n",
    "# Normalize Pawpularity to a range between 0 and 1\n",
    "train_df['Pawpularity'] = train_df['Pawpularity'] / 100.0  # [0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img, size=(224, 224)):\n",
    "    return img.resize(size)\n",
    "\n",
    "def normalize_image(img):\n",
    "    img_array = np.array(img) / 255.0\n",
    "    return Image.fromarray((img_array * 255).astype(np.uint8))  # Convert back to image if needed\n",
    "\n",
    "def random_flip(img):\n",
    "    if random.random() > 0.5:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n",
    "\n",
    "def random_vertical_flip(img):\n",
    "    if random.random() > 0.5:\n",
    "        return img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return img\n",
    "\n",
    "def random_rotation(img, max_angle=10): \n",
    "    angle = random.uniform(-max_angle, max_angle)\n",
    "    return img.rotate(angle)\n",
    "\n",
    "def color_jitter(img, brightness=0.1, contrast=0.1, saturation=0.1):  \n",
    "    img = ImageEnhance.Brightness(img).enhance(1 + random.uniform(-brightness, brightness))\n",
    "    img = ImageEnhance.Contrast(img).enhance(1 + random.uniform(-contrast, contrast))\n",
    "    img = ImageEnhance.Color(img).enhance(1 + random.uniform(-saturation, saturation))\n",
    "    return img\n",
    "\n",
    "def gaussian_blur(img, radius=0.5): \n",
    "    return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "def random_grayscale(img, p=0.1): \n",
    "    if random.random() < p:\n",
    "        return img.convert(\"L\").convert(\"RGB\")  # Convert to grayscale and back to RGB\n",
    "    return img\n",
    "\n",
    "def random_noise(img, noise_factor=0.01):  \n",
    "    img_array = np.array(img)\n",
    "    noise = np.random.normal(0, noise_factor * 255, img_array.shape).astype(np.uint8)\n",
    "    noisy_img_array = np.clip(img_array + noise, 0, 255)\n",
    "    return Image.fromarray(noisy_img_array.astype(np.uint8))\n",
    "\n",
    "def random_perspective(img, distortion=0.1): \n",
    "    width, height = img.size\n",
    "    x_shift = distortion * width\n",
    "    y_shift = distortion * height\n",
    "\n",
    "    coeffs = [\n",
    "        x_shift, y_shift,\n",
    "        width - x_shift, y_shift,\n",
    "        width - x_shift, height - y_shift,\n",
    "        x_shift, height - y_shift,\n",
    "    ]\n",
    "    return img.transform(img.size, Image.QUAD, coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold rows for the new CSV file\n",
    "augmented_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912/9912 [24:40<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def augment_images(train_df, img_dir, output_img_dir):\n",
    "    if not os.path.exists(output_img_dir):\n",
    "        os.makedirs(output_img_dir)\n",
    "\n",
    "    for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "        original_id = row['Id']\n",
    "        pawpularity_score = row['Pawpularity']\n",
    "        # Include additional columns to copy over\n",
    "        subject_focus = row['Subject Focus']\n",
    "        eyes = row['Eyes']\n",
    "        face = row['Face']\n",
    "        near = row['Near']\n",
    "        action = row['Action']\n",
    "        accessory = row['Accessory']\n",
    "        group = row['Group']\n",
    "        collage = row['Collage']\n",
    "        human = row['Human']\n",
    "        occlusion = row['Occlusion']\n",
    "        info = row['Info']\n",
    "        blur = row['Blur']\n",
    "\n",
    "        img_path = os.path.join(img_dir, f\"{original_id}.jpg\")\n",
    "\n",
    "        # Append the original row (for the unaugmented image)\n",
    "        augmented_rows.append(row.to_dict())\n",
    "\n",
    "        # Open and transform the image\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Preprocessing: Resize and normalize\n",
    "                original_resized = resize_image(img)\n",
    "                original_resized = normalize_image(original_resized)\n",
    "                original_resized.save(os.path.join(output_img_dir, f\"{original_id}.jpg\"))\n",
    "\n",
    "                # Augment and save\n",
    "                for i in range(5):  # Number of augmented versions per image\n",
    "                    augmented_img = resize_image(img)\n",
    "                    augmented_img = normalize_image(augmented_img)\n",
    "                    augmented_img = random_flip(augmented_img)\n",
    "                    augmented_img = random_vertical_flip(augmented_img)\n",
    "                    augmented_img = random_rotation(augmented_img)\n",
    "                    augmented_img = color_jitter(augmented_img)\n",
    "                    augmented_img = gaussian_blur(augmented_img)\n",
    "                    augmented_img = random_grayscale(augmented_img)\n",
    "                    augmented_img = random_noise(augmented_img)\n",
    "                    augmented_img = random_perspective(augmented_img)\n",
    "\n",
    "                    # Save the augmented image with a unique filename\n",
    "                    augmented_id = f\"{original_id}_aug{i}\"\n",
    "                    augmented_img.save(os.path.join(output_img_dir, f\"{augmented_id}.jpg\"))\n",
    "\n",
    "                    # Add a row for the augmented image in the new CSV data\n",
    "                    augmented_rows.append({\n",
    "                        'Id': augmented_id,\n",
    "                        'Pawpularity': pawpularity_score,\n",
    "                        'Subject Focus': subject_focus,\n",
    "                        'Eyes': eyes,\n",
    "                        'Face': face,\n",
    "                        'Near': near,\n",
    "                        'Action': action,\n",
    "                        'Accessory': accessory,\n",
    "                        'Group': group,\n",
    "                        'Collage': collage,\n",
    "                        'Human': human,\n",
    "                        'Occlusion': occlusion,\n",
    "                        'Info': info,\n",
    "                        'Blur': blur,\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "\n",
    "# Call the function to perform augmentations\n",
    "augment_images(train_df, img_dir, output_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to: augmented_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine augmented data with preprocessed CSV\n",
    "if augmented_rows:\n",
    "    # Create a DataFrame from the augmented rows\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    \n",
    "    # Load existing preprocessed CSV\n",
    "    if os.path.exists(output_csv_file):\n",
    "        existing_df = pd.read_csv(output_csv_file)\n",
    "        \n",
    "        # Combine the two DataFrames\n",
    "        combined_df = pd.concat([existing_df, augmented_df], ignore_index=True)\n",
    "    else:\n",
    "        # If the CSV does not exist, use only the augmented DataFrame\n",
    "        combined_df = augmented_df\n",
    "\n",
    "    # Save the combined data back to the preprocessed CSV file\n",
    "    combined_df.to_csv(output_csv_file, index=False)\n",
    "    print(\"Combined data saved to:\", output_csv_file)\n",
    "else:\n",
    "    print(\"No augmented data to combine.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
